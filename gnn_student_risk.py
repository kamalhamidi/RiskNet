"""
================================================================================
D√âTECTION DES √âTUDIANTS √Ä RISQUE VIA GRAPH NEURAL NETWORKS
Graph Neural Network Architecture for Academic Risk Detection
================================================================================

Auteur: Advanced AI Research
Date: 2026
Niveau: Recherche Master/PhD

CONTEXTE SCIENTIFIQUE
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
Ce projet impl√©mente une architecture Graph Neural Network pour la d√©tection
pr√©coce des √©tudiants √† risque d'√©chec acad√©mique.

FORMULATION MATH√âMATIQUE
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

1. REPR√âSENTATION EN GRAPHE
   - G = (V, E, X) o√π:
     * V = ensemble de n≈ìuds (√©tudiants), |V| = N
     * E = ensemble d'ar√™tes (relations de similarit√©)
     * X ‚àà ‚Ñù^(N√óD) = matrice d'attributs des n≈ìuds (features)

2. MATRICES CL√âS
   - A ‚àà ‚Ñù^(N√óN) = matrice d'adjacence pond√©r√©e
   - D ‚àà ‚Ñù^(N√óN) = matrice de degr√©s (D_ii = Œ£_j A_ij)
   - √É = D^(-1/2) A D^(-1/2) = normalization symm√©trique

3. COUCHE GCN (Graph Convolutional Network)
   H^(l+1) = œÉ(√É H^(l) W^(l))
   o√π:
   - H^(l) ‚àà ‚Ñù^(N√óD_l) = activation couche l
   - W^(l) ‚àà ‚Ñù^(D_l √ó D_(l+1)) = poids
   - œÉ = activation ReLU/ELU
   - √É = D^(-1/2) A D^(-1/2) (renormalization ChebNet)

4. COUCHE ATTENTION (GAT)
   Œ±_ij = softmax_j(LeakyReLU(a^T[W h_i || W h_j]))
   h_i' = œÉ(Œ£_j Œ±_ij W h_j)
   o√π:
   - Œ±_ij ‚àà [0,1] = coefficient d'attention
   - a ‚àà ‚Ñù^(2D) = vecteur d'attention apprenable
   - || = concat√©nation

5. FONCTION DE PERTE AVEC WEIGHTS (Class Imbalance)
   L = -Œ£_i [w_pos ¬∑ y_i ¬∑ log(≈∑_i) + w_neg ¬∑ (1-y_i) ¬∑ log(1-≈∑_i)]
   o√π:
   - w_pos = N_neg / (N_pos + N_neg) (poids positif)
   - w_neg = N_pos / (N_pos + N_neg) (poids n√©gatif)

6. M√âTRIQUE ROC-AUC
   AUC = ‚à´_0^1 TPR(FPR) dFPR
   avec validation robuste par validation crois√©e stratifi√©e

================================================================================
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler, RobustScaler
from sklearn.model_selection import StratifiedKFold, train_test_split
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    roc_auc_score, roc_curve, confusion_matrix, classification_report,
    auc
)
from sklearn.manifold import TSNE
from scipy.spatial.distance import cdist
from scipy.sparse import csr_matrix
import warnings
warnings.filterwarnings('ignore')

# PyTorch et PyTorch Geometric
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch_geometric.data import Data, DataLoader
from torch_geometric.nn import GCNConv, GATConv, GraphConv, global_mean_pool
from torch_geometric.utils import to_undirected, add_self_loops, remove_self_loops

# Configuration globale
SEED = 42
DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
np.random.seed(SEED)
torch.manual_seed(SEED)
if torch.cuda.is_available():
    torch.cuda.manual_seed(SEED)

print(f"[INFO] Device utilis√©: {DEVICE}")
print(f"[INFO] CUDA disponible: {torch.cuda.is_available()}")


# ================================================================================
# SECTION 1: ANALYSE EXPLORATOIRE DES DONN√âES
# ================================================================================

class DataAnalyzer:
    """Analyse statistique et pr√©paration des donn√©es"""
    
    def __init__(self, csv_path: str):
        self.df = pd.read_csv(csv_path)
        self.features = None
        self.target = None
        
    def load_and_describe(self):
        """Charge et d√©crit le dataset"""
        print("\n" + "="*80)
        print("STATISTIQUES DESCRIPTIVES DU DATASET")
        print("="*80)
        
        print(f"\nüìä Shape du dataset: {self.df.shape}")
        print(f"\nüìã Colonnes: {self.df.columns.tolist()}")
        
        print("\n" + "-"*80)
        print("DISTRIBUTION DE LA VARIABLE CIBLE (risk_label)")
        print("-"*80)
        
        class_dist = self.df['risk_label'].value_counts()
        print(f"\nClasse 0 (Non √† risque): {class_dist[0]} ({100*class_dist[0]/len(self.df):.2f}%)")
        print(f"Classe 1 (√Ä risque):     {class_dist[1]} ({100*class_dist[1]/len(self.df):.2f}%)")
        
        # Class imbalance ratio
        imbalance_ratio = class_dist[1] / class_dist[0]
        print(f"\n‚ö†Ô∏è  Ratio de d√©s√©quilibre: 1:{1/imbalance_ratio:.2f}")
        print(f"    ‚Üí Approche: Pond√©ration des classes + Focal Loss")
        
        print("\n" + "-"*80)
        print("STATISTIQUES DES FEATURES")
        print("-"*80)
        print(self.df.describe())
        
        print("\n" + "-"*80)
        print("VALEURS MANQUANTES")
        print("-"*80)
        missing = self.df.isnull().sum()
        if missing.sum() == 0:
            print("‚úÖ Aucune valeur manquante")
        else:
            print(missing[missing > 0])
        
        return class_dist
    
    def correlation_analysis(self):
        """Analyse des corr√©lations"""
        print("\n" + "="*80)
        print("MATRICE DE CORR√âLATION")
        print("="*80)
        
        numeric_cols = self.df.select_dtypes(include=[np.number]).columns
        corr_matrix = self.df[numeric_cols].corr()
        
        print("\nCorr√©lations avec risk_label:")
        risk_corr = corr_matrix['risk_label'].sort_values(ascending=False)
        print(risk_corr)
        
        # Visualisation
        plt.figure(figsize=(10, 8))
        sns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap='coolwarm', 
                   center=0, square=True, cbar_kws={"shrink": 0.8})
        plt.title('Matrice de Corr√©lation - Features Acad√©miques', fontsize=14, fontweight='bold')
        plt.tight_layout()
        plt.savefig('/Users/mac/Desktop/KAMAL/EDUCATION/MSID/S3/TG/Project/new model/01_correlation_matrix.png', dpi=300)
        plt.close()
        
        return corr_matrix
    
    def prepare_features(self):
        """Pr√©pare les features pour le GNN"""
        print("\n" + "="*80)
        print("PR√âPARATION DES FEATURES")
        print("="*80)
        
        # S√©lection des features pertinentes
        feature_cols = ['G1', 'G2', 'G3', 'studytime', 'absences', 
                       'failures', 'progression', 'avg_score', 'engagement_score']
        
        X = self.df[feature_cols].values
        y = self.df['risk_label'].values
        
        # Normalisation robuste (r√©sistant aux outliers)
        scaler = RobustScaler()
        X_scaled = scaler.fit_transform(X)
        
        print(f"\n‚úÖ Features s√©lectionn√©es: {len(feature_cols)}")
        print(f"   Dimensionnalit√© des n≈ìuds: D_in = {X_scaled.shape[1]}")
        print(f"   Nombre de n≈ìuds (√©tudiants): N = {X_scaled.shape[0]}")
        
        self.features = X_scaled
        self.target = y
        
        return X_scaled, y, feature_cols


# ================================================================================
# SECTION 2: CONSTRUCTION DU GRAPHE
# ================================================================================

class GraphConstructor:
    """Construction intelligente du graphe acad√©mique
    
    STRAT√âGIE DE CONSTRUCTION D'AR√äTES
    ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    
    M√©thode: KNN pond√©r√© avec similarit√© multi-dimensionnelle
    
    1. CALCUL DE LA SIMILITUDE (Cosinus normalis√©e)
       sim(i, j) = X_i ¬∑ X_j / (||X_i|| ¬∑ ||X_j||)
    
    2. D√âTECTION KNN
       Pour chaque n≈ìud i, connecter aux K-plus-proches-voisins
    
    3. POND√âRATION (Distance Gaussienne)
       w_ij = exp(-d_ij¬≤ / œÉ¬≤)
       o√π d_ij = ‚àö(Œ£(x_ik - x_jk)¬≤) (distance euclidienne)
    
    4. SP√âCIFICATION DE œÉ
       œÉ = percentile_75(distances) (adaptatif au dataset)
    
    5. SEUILLAGE ADAPTATIF
       Garder uniquement w_ij > threshold_min pour r√©duire bruit
    """
    
    def __init__(self, features: np.ndarray, target: np.ndarray, k: int = 10):
        self.features = features
        self.target = target
        self.k = k
        self.N = len(features)
        
    def compute_edge_weights(self, method: str = 'gaussian'):
        """
        Calcule les poids des ar√™tes
        
        Args:
            method: 'gaussian' (d√©faut) ou 'cosine'
            
        Returns:
            edges (Tensor): [2, num_edges]
            weights (Tensor): [num_edges]
        """
        print("\n" + "="*80)
        print("CONSTRUCTION DU GRAPHE - ANALYSE D√âTAILL√âE")
        print("="*80)
        
        # 1. Calcul des distances euclidiennes
        print("\n[√âtape 1] Calcul de la matrice de distances...")
        distances = cdist(self.features, self.features, metric='euclidean')
        np.fill_diagonal(distances, np.inf)  # √âviter auto-boucles
        
        # 2. S√©lection KNN
        print(f"[√âtape 2] S√©lection des K={self.k} plus proches voisins...")
        knn_indices = np.argsort(distances, axis=1)[:, :self.k]
        
        # 3. Pond√©ration Gaussienne
        print("[√âtape 3] Pond√©ration Gaussienne des ar√™tes...")
        sigma = np.percentile(distances[distances != np.inf], 75)
        print(f"   œÉ calcul√© (75e percentile): {sigma:.4f}")
        
        edges = []
        weights = []
        
        for i in range(self.N):
            for j in knn_indices[i]:
                d_ij = distances[i, j]
                w_ij = np.exp(-(d_ij**2) / (2 * sigma**2))
                
                # Seuillage adaptatif
                if w_ij > 0.1:  # Threshold minimum pour √©liminer bruit
                    edges.append([i, j])
                    weights.append(w_ij)
        
        edges = np.array(edges).T
        weights = np.array(weights)
        
        # Normalisation des poids
        weights = weights / weights.max()
        
        print(f"\n‚úÖ Graphe construit:")
        print(f"   Nombre de n≈ìuds: {self.N}")
        print(f"   Nombre d'ar√™tes: {len(weights)}")
        print(f"   Densit√© du graphe: {2*len(weights)/(self.N*(self.N-1)):.4f}")
        print(f"   Degr√© moyen: {2*len(weights)/self.N:.2f}")
        
        # Statistics
        print(f"\nüìä Statistiques des poids:")
        print(f"   Min: {weights.min():.4f}")
        print(f"   Max: {weights.max():.4f}")
        print(f"   Moy: {weights.mean():.4f}")
        print(f"   Std: {weights.std():.4f}")
        
        return torch.LongTensor(edges), torch.FloatTensor(weights)
    
    def create_pyg_data(self, edges, weights):
        """Cr√©e un objet PyG Data"""
        x = torch.FloatTensor(self.features)
        y = torch.LongTensor(self.target)
        
        data = Data(
            x=x,
            edge_index=edges,
            edge_attr=weights.unsqueeze(-1),  # Poids comme attribut d'ar√™te
            y=y,
            num_nodes=self.N
        )
        
        return data


# ================================================================================
# SECTION 3: ARCHITECTURE GNN AVANC√âE
# ================================================================================

class HybridGNNModel(nn.Module):
    """
    Architecture GNN Hybride : GCN + GAT
    
    JUSTIFICATION TH√âORIQUE
    ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    
    Combinaison optimale pour ce probl√®me:
    
    1. GCN pour agr√©gation globale
       - Capture relations g√©n√©rales de similarit√©
       - Efficace computationnellement O(|E|)
    
    2. GAT pour attention locale
       - Poids adaptatifs par ar√™te
       - Capture relations critiques sp√©cifiques
       - Interpretabilit√© via coefficients d'attention
    
    3. Dropout & BatchNorm
       - Pr√©vention du surapprentissage
       - Acc√©l√©ration convergence
       - Robustesse num√©rique
    
    Architecture:
    ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    Input (D_in=9)
         ‚Üì
    GCN Layer 1 (D_in ‚Üí 64, ReLU) + Dropout(0.3)
         ‚Üì
    GAT Layer (64 ‚Üí 64, 8 heads, ELU) + Dropout(0.3)
         ‚Üì
    GCN Layer 2 (64 ‚Üí 32, ReLU) + Dropout(0.2)
         ‚Üì
    Output Layer (32 ‚Üí 1, Sigmoid)
         ‚Üì
    Binary Classification
    """
    
    def __init__(self, input_dim: int, hidden_dims: list = [64, 64, 32],
                 num_heads: int = 8, dropout: float = 0.3):
        super(HybridGNNModel, self).__init__()
        
        self.input_dim = input_dim
        self.hidden_dims = hidden_dims
        self.dropout_rate = dropout
        
        # Couche 1: GCN
        self.gcn1 = GCNConv(input_dim, hidden_dims[0])
        self.bn1 = nn.BatchNorm1d(hidden_dims[0])
        
        # Couche 2: GAT (multi-head attention)
        self.gat = GATConv(hidden_dims[0], hidden_dims[1], 
                          heads=num_heads, concat=False, dropout=dropout)
        self.bn2 = nn.BatchNorm1d(hidden_dims[1])
        
        # Couche 3: GCN
        self.gcn2 = GCNConv(hidden_dims[1], hidden_dims[2])
        self.bn3 = nn.BatchNorm1d(hidden_dims[2])
        
        # Couche de sortie
        self.fc = nn.Linear(hidden_dims[2], 1)
        
        # Dropout et activation
        self.dropout = nn.Dropout(dropout)
        self.elu = nn.ELU(alpha=1.0)
        self.relu = nn.ReLU()
        
    def forward(self, data):
        x, edge_index = data.x, data.edge_index
        
        # GCN Layer 1
        x = self.gcn1(x, edge_index)
        x = self.bn1(x)
        x = self.relu(x)
        x = self.dropout(x)
        
        # GAT Layer avec multi-head attention
        x = self.gat(x, edge_index)
        x = self.bn2(x)
        x = self.elu(x)
        x = self.dropout(x)
        
        # GCN Layer 2
        x = self.gcn2(x, edge_index)
        x = self.bn3(x)
        x = self.relu(x)
        x = self.dropout(x)
        
        # Output layer
        x = self.fc(x)
        x = torch.sigmoid(x)
        
        return x.squeeze()


# ================================================================================
# SECTION 4: GESTION DU CLASS IMBALANCE
# ================================================================================

class FocalLoss(nn.Module):
    """
    Focal Loss pour la gestion du class imbalance
    
    FORMULE MATH√âMATIQUE
    ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    
    Focal Loss = -Œ±_t ¬∑ (1 - p_t)^Œ≥ ¬∑ log(p_t)
    
    o√π:
    - Œ±_t = poids de la classe (w_pos ou w_neg)
    - p_t = probabilit√© pr√©dite
    - Œ≥ = facteur d'importance (focusing parameter)
      * Œ≥ ‚àà [0, 5], typiquement 2
      * Œ≥=0 : BCE standard
      * Œ≥‚Üë : focus sur exemples difficiles (hard negatives)
    
    Intuition:
    - Exemples faciles (p_t ‚âà 1) : perte ‚âà 0
    - Exemples durs (p_t ‚âà 0.5) : perte maximale
    - Pr√©vient la domination des exemples faciles et nombreux
    """
    
    def __init__(self, alpha: float = 0.25, gamma: float = 2.0):
        super(FocalLoss, self).__init__()
        self.alpha = alpha
        self.gamma = gamma
        
    def forward(self, inputs, targets, class_weights=None):
        # BCE avec poids de classe
        bce_loss = F.binary_cross_entropy(inputs, targets, reduction='none')
        
        # Focal term
        p_t = torch.where(targets == 1, inputs, 1 - inputs)
        focal_weight = (1 - p_t) ** self.gamma
        
        # Focal loss
        focal_loss = self.alpha * focal_weight * bce_loss
        
        if class_weights is not None:
            focal_loss = focal_loss * class_weights
        
        return focal_loss.mean()


class WeightedBCELoss(nn.Module):
    """BCE Loss pond√©r√©e par classe"""
    
    def __init__(self, pos_weight: float = 1.0):
        super(WeightedBCELoss, self).__init__()
        self.pos_weight = pos_weight
        
    def forward(self, inputs, targets):
        return F.binary_cross_entropy(inputs, targets, 
                                     weight=self.compute_weights(targets),
                                     reduction='mean')
    
    def compute_weights(self, targets):
        weights = torch.where(targets == 1, 
                            torch.tensor(self.pos_weight, device=targets.device),
                            torch.tensor(1.0, device=targets.device))
        return weights


# ================================================================================
# SECTION 5: ENTRA√éNEMENT ET VALIDATION
# ================================================================================

class GNNTrainer:
    """Pipeline d'entra√Ænement complet avec validation rigoureuse"""
    
    def __init__(self, model: nn.Module, device: torch.device,
                 learning_rate: float = 0.001, weight_decay: float = 5e-4):
        self.model = model.to(device)
        self.device = device
        self.optimizer = optim.Adam(model.parameters(), 
                                    lr=learning_rate, weight_decay=weight_decay)
        
        # Schedulers pour am√©liorer convergence
        self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(
            self.optimizer, mode='max', factor=0.5, patience=10, 
            min_lr=1e-6
        )
        
        self.train_losses = []
        self.val_metrics = {'auc': [], 'f1': [], 'loss': []}
        self.best_auc = 0.0
        self.patience_counter = 0
        
    def compute_class_weights(self, y):
        """Calcule les poids de classe"""
        unique, counts = np.unique(y, return_counts=True)
        weight_dict = {u: (1.0 / c) * (sum(counts) / len(counts)) 
                      for u, c in zip(unique, counts)}
        return weight_dict
    
    def train_epoch(self, train_data, loss_fn):
        """Entra√Æne une √©poque"""
        self.model.train()
        
        train_data = train_data.to(self.device)
        self.optimizer.zero_grad()
        
        # Forward pass
        out = self.model(train_data)
        targets = train_data.y.float()
        
        # Loss
        loss = loss_fn(out, targets)
        
        # Backward pass
        loss.backward()
        torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
        self.optimizer.step()
        
        return loss.item()
    
    @torch.no_grad()
    def evaluate(self, data, loss_fn):
        """√âvalue le mod√®le"""
        self.model.eval()
        
        data = data.to(self.device)
        out = self.model(data)
        targets = data.y.float()
        
        # Loss
        loss = loss_fn(out, targets).item()
        
        # M√©triques
        out_cpu = out.cpu().numpy()
        targets_cpu = targets.cpu().numpy()
        
        y_pred = (out_cpu > 0.5).astype(int)
        auc = roc_auc_score(targets_cpu, out_cpu)
        f1 = f1_score(targets_cpu, y_pred)
        
        return {'auc': auc, 'f1': f1, 'loss': loss, 'out': out_cpu, 'y': targets_cpu}
    
    def train(self, train_data, val_data, epochs: int = 100,
              loss_fn_name: str = 'weighted_bce', early_stopping: bool = True):
        """
        Entra√Æne le mod√®le
        
        Args:
            train_data: PyG Data object
            val_data: PyG Data object
            epochs: nombre d'√©pochs
            loss_fn_name: 'weighted_bce' ou 'focal'
            early_stopping: activation de l'early stopping
        """
        
        # S√©lection de la fonction de perte
        if loss_fn_name == 'focal':
            loss_fn = FocalLoss(alpha=0.25, gamma=2.0).to(self.device)
        else:
            # Poids de classe
            class_weights = self.compute_class_weights(train_data.y.numpy())
            pos_weight = class_weights[0] / class_weights[1]
            loss_fn = WeightedBCELoss(pos_weight=pos_weight).to(self.device)
        
        print("\n" + "="*80)
        print(f"ENTRA√éNEMENT DU MOD√àLE ({loss_fn_name.upper()})")
        print("="*80)
        
        for epoch in range(1, epochs + 1):
            # Train
            train_loss = self.train_epoch(train_data, loss_fn)
            
            # Validation
            val_metrics = self.evaluate(val_data, loss_fn)
            
            # Store
            self.train_losses.append(train_loss)
            self.val_metrics['auc'].append(val_metrics['auc'])
            self.val_metrics['f1'].append(val_metrics['f1'])
            self.val_metrics['loss'].append(val_metrics['loss'])
            
            # Learning rate scheduling
            self.scheduler.step(val_metrics['auc'])
            
            # Early stopping
            if val_metrics['auc'] > self.best_auc:
                self.best_auc = val_metrics['auc']
                self.patience_counter = 0
                # Sauvegarde du meilleur mod√®le
                torch.save(self.model.state_dict(), 
                          '/Users/mac/Desktop/KAMAL/EDUCATION/MSID/S3/TG/Project/new model/best_model.pt')
            else:
                self.patience_counter += 1
            
            if epoch % 10 == 0 or epoch == 1:
                print(f"Epoch {epoch:3d}/{epochs} | Train Loss: {train_loss:.4f} | "
                      f"Val AUC: {val_metrics['auc']:.4f} | Val F1: {val_metrics['f1']:.4f} | "
                      f"Patience: {self.patience_counter}")
            
            if early_stopping and self.patience_counter >= 20:
                print(f"\n‚èπÔ∏è  Early stopping at epoch {epoch}")
                break
        
        # Load best model
        self.model.load_state_dict(torch.load(
            '/Users/mac/Desktop/KAMAL/EDUCATION/MSID/S3/TG/Project/new model/best_model.pt'))
        
        return self.train_losses, self.val_metrics


# ================================================================================
# SECTION 6: √âVALUATION ROBUSTE
# ================================================================================

class RobustEvaluator:
    """√âvaluation compl√®te et robuste du mod√®le"""
    
    def __init__(self, model: nn.Module, device: torch.device):
        self.model = model
        self.device = device
        
    @torch.no_grad()
    def predict(self, data):
        """Pr√©dictions"""
        self.model.eval()
        data = data.to(self.device)
        out = self.model(data)
        return out.cpu().numpy().flatten()
    
    def compute_all_metrics(self, y_true, y_pred_proba):
        """Calcule toutes les m√©triques"""
        y_pred = (y_pred_proba > 0.5).astype(int)
        
        metrics = {
            'accuracy': accuracy_score(y_true, y_pred),
            'precision': precision_score(y_true, y_pred, zero_division=0),
            'recall': recall_score(y_true, y_pred, zero_division=0),
            'f1': f1_score(y_true, y_pred, zero_division=0),
            'auc': roc_auc_score(y_true, y_pred_proba),
            'specificity': confusion_matrix(y_true, y_pred).ravel()[0] / (y_true == 0).sum(),
        }
        
        return metrics, y_pred
    
    def print_report(self, y_true, y_pred_proba, set_name: str = "Test"):
        """Rapport d'√©valuation d√©taill√©"""
        metrics, y_pred = self.compute_all_metrics(y_true, y_pred_proba)
        
        print("\n" + "="*80)
        print(f"RAPPORT D'√âVALUATION - {set_name.upper()}")
        print("="*80)
        
        print(f"\nüìä M√©triques de Classification:")
        print(f"   Accuracy:    {metrics['accuracy']:.4f}")
        print(f"   Precision:   {metrics['precision']:.4f}  (TP / (TP + FP))")
        print(f"   Recall:      {metrics['recall']:.4f}  (TP / (TP + FN))")
        print(f"   Specificity: {metrics['specificity']:.4f}  (TN / (TN + FP))")
        print(f"   F1-Score:    {metrics['f1']:.4f}  (harmonic mean)")
        print(f"   ROC-AUC:     {metrics['auc']:.4f}  ‚ú®")
        
        # Confusion matrix
        cm = confusion_matrix(y_true, y_pred)
        print(f"\nüìã Matrice de Confusion:")
        print(f"   TN: {cm[0,0]:5d}  |  FP: {cm[0,1]:5d}")
        print(f"   FN: {cm[1,0]:5d}  |  TP: {cm[1,1]:5d}")
        
        print(f"\nüìù Classification Report:")
        print(classification_report(y_true, y_pred, 
                                   target_names=['Non-risque', '√Ä risque']))
        
        return metrics
    
    def plot_roc_curve(self, y_true, y_pred_proba, save_path: str = None):
        """Trace la courbe ROC"""
        fpr, tpr, thresholds = roc_curve(y_true, y_pred_proba)
        roc_auc = auc(fpr, tpr)
        
        plt.figure(figsize=(10, 8))
        plt.plot(fpr, tpr, color='darkorange', lw=3, 
                label=f'ROC curve (AUC = {roc_auc:.4f})')
        plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', 
                label='Random Classifier')
        plt.xlim([0.0, 1.0])
        plt.ylim([0.0, 1.05])
        plt.xlabel('False Positive Rate', fontsize=12, fontweight='bold')
        plt.ylabel('True Positive Rate', fontsize=12, fontweight='bold')
        plt.title('Courbe ROC - D√©tection √âtudiants √† Risque', 
                 fontsize=14, fontweight='bold')
        plt.legend(loc="lower right", fontsize=11)
        plt.grid(alpha=0.3)
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=300)
        plt.show()
    
    def plot_confusion_matrix(self, y_true, y_pred, save_path: str = None):
        """Trace la matrice de confusion"""
        cm = confusion_matrix(y_true, y_pred)
        
        plt.figure(figsize=(8, 6))
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                   xticklabels=['Non-risque', '√Ä risque'],
                   yticklabels=['Non-risque', '√Ä risque'],
                   cbar_kws={"shrink": 0.8})
        plt.title('Matrice de Confusion', fontsize=14, fontweight='bold')
        plt.ylabel('Vrai Label', fontsize=12, fontweight='bold')
        plt.xlabel('Pr√©diction', fontsize=12, fontweight='bold')
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=300)
        plt.show()


# ================================================================================
# SECTION 7: VISUALISATION DES EMBEDDINGS
# ================================================================================

class EmbeddingVisualizer:
    """Visualisation des node embeddings via t-SNE"""
    
    def __init__(self, model: nn.Module, device: torch.device):
        self.model = model
        self.device = device
        
    @torch.no_grad()
    def extract_embeddings(self, data, layer: int = -2):
        """Extrait les embeddings avant la couche de sortie"""
        self.model.eval()
        data = data.to(self.device)
        
        # Forward jusqu'√† la couche -2
        x = data.x
        edge_index = data.edge_index
        
        # GCN Layer 1
        x = self.model.gcn1(x, edge_index)
        x = self.model.bn1(x)
        x = self.model.relu(x)
        
        # GAT Layer
        x = self.model.gat(x, edge_index)
        x = self.model.bn2(x)
        x = self.model.elu(x)
        
        # GCN Layer 2
        x = self.model.gcn2(x, edge_index)
        x = self.model.bn3(x)
        x = self.model.relu(x)
        
        return x.cpu().numpy()
    
    def visualize_tsne(self, embeddings, labels, save_path: str = None):
        """Visualise les embeddings en t-SNE"""
        print("\n[INFO] R√©duction t-SNE en cours (peut prendre quelques secondes)...")
        tsne = TSNE(n_components=2, perplexity=30, max_iter=1000, 
                   random_state=SEED, verbose=0)
        embeddings_2d = tsne.fit_transform(embeddings)
        
        plt.figure(figsize=(12, 9))
        scatter = plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1],
                            c=labels, cmap='RdYlGn_r', s=100, alpha=0.7,
                            edgecolors='black', linewidth=0.5)
        
        cbar = plt.colorbar(scatter, label='Risk Label')
        cbar.set_label('Risk Label (0=Safe, 1=At Risk)', fontsize=11)
        
        plt.title('t-SNE Visualization - Node Embeddings', 
                 fontsize=14, fontweight='bold')
        plt.xlabel('t-SNE Component 1', fontsize=12, fontweight='bold')
        plt.ylabel('t-SNE Component 2', fontsize=12, fontweight='bold')
        plt.grid(alpha=0.3)
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=300)
        plt.show()
        
        return embeddings_2d


# ================================================================================
# SECTION 8: VALIDATION CROIS√âE STRATIFI√âE
# ================================================================================

class StratifiedCrossValidator:
    """Validation crois√©e stratifi√©e pour √©valuation robuste"""
    
    def __init__(self, n_splits: int = 5, random_state: int = SEED):
        self.n_splits = n_splits
        self.skf = StratifiedKFold(n_splits=n_splits, shuffle=True, 
                                   random_state=random_state)
        self.fold_results = []
        
    def run_cv(self, graph_data, model_class, epochs: int = 100, 
              device: torch.device = DEVICE):
        """Ex√©cute la validation crois√©e"""
        
        print("\n" + "="*80)
        print(f"VALIDATION CROIS√âE STRATIFI√âE ({self.n_splits} folds)")
        print("="*80)
        
        indices = np.arange(len(graph_data.y))
        y = graph_data.y.numpy()
        
        for fold, (train_idx, val_idx) in enumerate(self.skf.split(indices, y), 1):
            print(f"\n‚îÄ‚îÄ‚îÄ Fold {fold}/{self.n_splits} ‚îÄ‚îÄ‚îÄ")
            
            # Split donn√©es
            train_mask = torch.zeros(len(y), dtype=torch.bool)
            val_mask = torch.zeros(len(y), dtype=torch.bool)
            train_mask[train_idx] = True
            val_mask[val_idx] = True
            
            # Cr√©er sous-ensembles
            train_data = Data(
                x=graph_data.x,
                edge_index=graph_data.edge_index,
                edge_attr=graph_data.edge_attr,
                y=graph_data.y,
                train_mask=train_mask,
                val_mask=val_mask
            )
            val_data = Data(
                x=graph_data.x,
                edge_index=graph_data.edge_index,
                edge_attr=graph_data.edge_attr,
                y=graph_data.y,
                train_mask=train_mask,
                val_mask=val_mask
            )
            
            # Mod√®le et entra√Ænement
            model = model_class(input_dim=graph_data.x.shape[1]).to(device)
            trainer = GNNTrainer(model, device)
            trainer.train(train_data, val_data, epochs=epochs, 
                         loss_fn_name='weighted_bce', early_stopping=True)
            
            # √âvaluation
            evaluator = RobustEvaluator(model, device)
            y_pred = evaluator.predict(val_data)
            y_true = graph_data.y[val_idx].numpy()
            
            metrics, _ = evaluator.compute_all_metrics(y_true, y_pred)
            self.fold_results.append(metrics)
            
            print(f"   Fold AUC: {metrics['auc']:.4f} | F1: {metrics['f1']:.4f}")
        
        self._print_cv_summary()
    
    def _print_cv_summary(self):
        """R√©sum√© de la validation crois√©e"""
        print("\n" + "="*80)
        print("R√âSUM√â VALIDATION CROIS√âE")
        print("="*80)
        
        df_results = pd.DataFrame(self.fold_results)
        
        print("\nüìä Moyennes et √©carts-types:")
        for col in df_results.columns:
            mean = df_results[col].mean()
            std = df_results[col].std()
            print(f"   {col:12s}: {mean:.4f} ¬± {std:.4f}")


print("\n‚úÖ Module GNN Student Risk charg√© avec succ√®s!")
