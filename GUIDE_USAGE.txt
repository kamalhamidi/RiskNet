â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
GUIDE D'UTILISATION COMPLET - GNN STUDENT RISK DETECTION
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

TABLE DES MATIÃˆRES:
  1. Installation
  2. Structure du Projet
  3. Utilisation Basique
  4. Tuning AvancÃ©
  5. DÃ©pannage
  6. Production Deployment

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
1. INSTALLATION
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

PRÃ‰-REQUIS:
  â€¢ Python â‰¥ 3.8
  â€¢ pip ou conda
  â€¢ ~2 GB RAM libre
  â€¢ GPU (optionnel, mais recommandÃ© pour datasets > 10k Ã©tudiants)

Ã‰TAPE 1: Cloner/TÃ©lÃ©charger le projet
  ```
  cd /Users/mac/Desktop/KAMAL/EDUCATION/MSID/S3/TG/Project/new\ model
  ```

Ã‰TAPE 2: CrÃ©er environnement virtuel
  ```bash
  python -m venv venv
  source venv/bin/activate  # Mac/Linux
  # ou
  venv\Scripts\activate  # Windows
  ```

Ã‰TAPE 3: Installer dÃ©pendances
  ```bash
  pip install torch==2.0.0
  pip install torch-geometric
  pip install numpy pandas scikit-learn matplotlib seaborn scipy
  ```

Ã‰TAPE 4: VÃ©rifier installation
  ```python
  python -c "import torch; import torch_geometric; print('âœ… OK')"
  ```

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
2. STRUCTURE DU PROJET
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

new\ model/
  â”œâ”€â”€ students.csv                    # Dataset acadÃ©mique (395 Ã©tudiants)
  â”œâ”€â”€ script.ipynb                    # Notebook Jupyter exÃ©cutable
  â”œâ”€â”€ gnn_student_risk.py             # Modules principaux
  â”œâ”€â”€ DOCUMENTATION_COMPLETE.md       # ThÃ©orie complÃ¨te
  â”œâ”€â”€ FINAL_RESULTS_REPORT.txt        # RÃ©sultats finaux
  â”œâ”€â”€ GUIDE_USAGE.txt                 # Ce fichier
  â”‚
  â”œâ”€â”€ models/
  â”‚   â””â”€â”€ best_model.pt               # Poids du meilleur modÃ¨le (gÃ©nÃ©rÃ©)
  â”‚
  â””â”€â”€ results/
      â”œâ”€â”€ 01_correlation_matrix.png
      â”œâ”€â”€ 02_training_curves.png
      â”œâ”€â”€ 03_roc_curve.png
      â”œâ”€â”€ 04_confusion_matrix.png
      â”œâ”€â”€ 05_embeddings_tsne.png
      â”œâ”€â”€ 06_train_test_comparison.png
      â””â”€â”€ 07_error_analysis.png

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
3. UTILISATION BASIQUE
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

OPTION A: Jupyter Notebook (RecommandÃ© pour exploration)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

  1. DÃ©marrer Jupyter:
     ```bash
     jupyter notebook
     ```

  2. Ouvrir: script.ipynb

  3. ExÃ©cuter cellules sÃ©quentiellement (Shift + Enter)

  4. Visualiser rÃ©sultats interactifs

OPTION B: Script Python Standalone (RecommandÃ© pour production)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

  1. CrÃ©er fichier: train_model.py
  
  ```python
  from gnn_student_risk import *
  
  # 1. Charger donnÃ©es
  analyzer = DataAnalyzer('students.csv')
  X_scaled, y, features = analyzer.prepare_features()
  
  # 2. Construire graphe
  gc = GraphConstructor(X_scaled, y, k=10)
  edges, weights = gc.compute_edge_weights()
  data = gc.create_pyg_data(edges, weights)
  
  # 3. Split train/test
  train_idx, test_idx, y_train, y_test = train_test_split(
      np.arange(len(y)), y, train_size=0.8, 
      stratify=y, random_state=42
  )
  
  # 4. ModÃ¨le
  model = HybridGNNModel(input_dim=9, hidden_dims=[64,64,32])
  
  # 5. EntraÃ®nement
  trainer = GNNTrainer(model, device='cuda')
  trainer.train(train_data, test_data, epochs=150)
  
  # 6. Ã‰valuation
  evaluator = RobustEvaluator(model, device='cuda')
  metrics = evaluator.print_report(y_test, y_pred_proba)
  
  # 7. Sauvegarder modÃ¨le
  torch.save(model.state_dict(), 'best_model.pt')
  ```

  2. ExÃ©cuter:
     ```bash
     python train_model.py
     ```

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
4. TUNING AVANCÃ‰
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

4.1 TUNING MANUEL DES HYPERPARAMÃˆTRES
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Parameters Ã  ajuster:

  A. Graphe:
     â€¢ k (nombre voisins):
       - Valeur actuelle: 10
       - Range recommandÃ©e: [5, 15]
       - Augmenter â†’ connexions plus nombreuses â†’ lissage plus fort
       - Diminuer â†’ connexions plus sÃ©lectives â†’ variation locale
     
     â€¢ sigma (bandwidth Gaussienne):
       - Actuel: percentile 75
       - Alternatives: percentile 50, 60, 90
       - Impacte pondÃ©ration des arÃªtes
  
  B. Architecture:
     â€¢ hidden_dims:
       - Actuel: [64, 64, 32]
       - Alternatives: [32, 32, 16], [128, 128, 64]
       - Plus grand â†’ plus expressif (risque overfitting)
       - Plus petit â†’ plus simple (underfitting)
     
     â€¢ num_heads (GAT):
       - Actuel: 8
       - Alternatives: 4, 16
       - Plus d'heads â†’ plus de perspectives
     
     â€¢ dropout:
       - Actuel: [0.3, 0.3, 0.2]
       - Alternatives: [0.2, 0.2, 0.1] ou [0.5, 0.5, 0.3]
       - Plus Ã©levÃ© â†’ plus de rÃ©gularisation
  
  C. Optimisation:
     â€¢ learning_rate:
       - Actuel: 0.001
       - Alternatives: [1e-4, 0.01]
       - Plus grand â†’ convergence rapide (instabilitÃ©)
       - Plus petit â†’ convergence lente (stabilitÃ©)
     
     â€¢ weight_decay (L2 rÃ©gularisation):
       - Actuel: 5e-4
       - Alternatives: [1e-5, 1e-3]
     
     â€¢ epochs:
       - Actuel: 150
       - Ã‰valuer convergence: voir training curves
     
     â€¢ patience (early stopping):
       - Actuel: 20
       - Alternatives: [10, 30]

4.2 OPTIMISATION AUTOMATIQUE (OPTUNA)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Installation:
  ```bash
  pip install optuna
  ```

Exemple:
  ```python
  import optuna
  from optuna.samplers import TPESampler
  
  def objective(trial):
      # SuggÃ©rer hyperparamÃ¨tres
      k = trial.suggest_int('k', 5, 15)
      hidden_dim = trial.suggest_int('hidden_dim', 32, 128, step=32)
      dropout = trial.suggest_float('dropout', 0.1, 0.5)
      lr = trial.suggest_float('lr', 1e-4, 1e-2, log=True)
      
      # Construire modÃ¨le
      model = HybridGNNModel(
          input_dim=9,
          hidden_dims=[hidden_dim, hidden_dim, hidden_dim//2],
          dropout=dropout
      )
      
      # EntraÃ®ner et Ã©valuer
      trainer = GNNTrainer(model, device='cuda', learning_rate=lr)
      trainer.train(train_data, test_data, epochs=100)
      evaluator = RobustEvaluator(model, device='cuda')
      
      # Retourner mÃ©trique d'optimisation
      auc = evaluator.compute_all_metrics(y_test, y_pred)[0]['auc']
      return auc
  
  # Lancer optimisation
  study = optuna.create_study(
      direction='maximize',
      sampler=TPESampler(seed=42)
  )
  study.optimize(objective, n_trials=50, n_jobs=1)
  
  # Afficher meilleurs paramÃ¨tres
  print("Best params:", study.best_params)
  print("Best AUC:", study.best_value)
  ```

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
5. DÃ‰PANNAGE
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

PROBLÃˆME: ModuleNotFoundError: No module named 'torch_geometric'
SOLUTION:
  ```bash
  pip install --upgrade torch-geometric
  pip install torch-scatter torch-sparse
  ```

PROBLÃˆME: CUDA OutOfMemory
SOLUTION:
  â€¢ RÃ©duire batch size (option dans DataLoader)
  â€¢ Utiliser device='cpu'
  â€¢ RÃ©duire hidden_dims

PROBLÃˆME: ModÃ¨le diverge (loss â†’ inf)
SOLUTION:
  â€¢ RÃ©duire learning_rate (try 1e-4)
  â€¢ Activer gradient clipping (dÃ©jÃ  dans code)
  â€¢ Normaliser features (dÃ©jÃ  fait avec RobustScaler)

PROBLÃˆME: Poor performance sur donnÃ©es rÃ©elles
SOLUTION:
  â€¢ VÃ©rifier formato donnÃ©es (shape, dtype)
  â€¢ Augmenter epochs si courbes pas stabilisÃ©es
  â€¢ Tuner hyperparamÃ¨tres avec Optuna
  â€¢ VÃ©rifier class imbalance (peut nÃ©cessiter Focal Loss)

PROBLÃˆME: Val AUC ne s'amÃ©liore plus
SOLUTION:
  â€¢ ModÃ¨le convergÃ© â†’ arrÃªt normal
  â€¢ Si dÃ©sire amÃ©lioration: Ensemble methods
  â€¢ Ou: Acquire plus de donnÃ©es

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
6. PRODUCTION DEPLOYMENT
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

6.1 SAUVEGARDE DU MODÃˆLE
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

  ```python
  import pickle
  
  # Sauvegarder modÃ¨le
  torch.save(model.state_dict(), 'models/student_risk_gnn.pt')
  
  # Sauvegarder scaler pour normalization
  pickle.dump(scaler, open('models/scaler.pkl', 'wb'))
  
  # Sauvegarder mÃ©tadonnÃ©es
  metadata = {
      'input_dim': 9,
      'hidden_dims': [64, 64, 32],
      'num_heads': 8,
      'dropout': 0.3,
      'train_auc': 0.9794,
      'test_auc': 0.9900,
      'threshold': 0.45  # Seuil de dÃ©cision recommandÃ©
  }
  pickle.dump(metadata, open('models/metadata.pkl', 'wb'))
  ```

6.2 CHARGER & PRÃ‰DIRE
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

  ```python
  import pickle
  
  # Charger modÃ¨le
  model = HybridGNNModel(input_dim=9, hidden_dims=[64,64,32])
  model.load_state_dict(torch.load('models/student_risk_gnn.pt'))
  model.eval()
  
  # Charger scaler
  scaler = pickle.load(open('models/scaler.pkl', 'rb'))
  
  # Charger mÃ©tadonnÃ©es
  metadata = pickle.load(open('models/metadata.pkl', 'rb'))
  threshold = metadata['threshold']
  
  # PrÃ©parer donnÃ©es nouvelles
  X_new = pd.read_csv('new_students.csv')
  X_new_scaled = scaler.transform(X_new)
  
  # Construire graphe pour new data (NOTE: refaire avec historique)
  gc = GraphConstructor(X_new_scaled, k=10)
  edges, weights = gc.compute_edge_weights()
  data_new = gc.create_pyg_data(edges, weights)
  
  # PrÃ©dire
  with torch.no_grad():
      y_pred_proba = model(data_new).cpu().numpy()
  
  # Appliquer seuil
  y_pred = (y_pred_proba > threshold).astype(int)
  
  # GÃ©nÃ©rer rapport
  results = pd.DataFrame({
      'student_id': X_new['student_id'],
      'risk_probability': y_pred_proba,
      'predicted_risk': y_pred,
      'intervention_needed': y_pred == 1
  })
  
  results.to_csv('predictions.csv', index=False)
  ```

6.3 API ENDPOINT (Flask)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

  ```python
  from flask import Flask, request, jsonify
  import pickle
  
  app = Flask(__name__)
  
  # Charger modÃ¨le au dÃ©marrage
  model = HybridGNNModel(input_dim=9)
  model.load_state_dict(torch.load('models/student_risk_gnn.pt'))
  scaler = pickle.load(open('models/scaler.pkl', 'rb'))
  
  @app.route('/predict', methods=['POST'])
  def predict():
      data = request.json
      
      # Parser input
      X = np.array([data.values()])
      X_scaled = scaler.transform(X)
      
      # PrÃ©dire
      with torch.no_grad():
          prob = model(X_scaled).item()
      
      return jsonify({
          'risk_probability': float(prob),
          'at_risk': prob > 0.45,
          'intervention_tier': (
              'urgent' if prob > 0.80 else
              'planned' if prob > 0.50 else
              'monitoring' if prob > 0.35 else
              'low_risk'
          )
      })
  
  if __name__ == '__main__':
      app.run(debug=False, port=5000)
  ```

  Usage:
  ```bash
  curl -X POST http://localhost:5000/predict \
    -H "Content-Type: application/json" \
    -d '{"G1": 10, "G2": 11, "G3": 12, ...}'
  ```

6.4 MONITORING EN PRODUCTION
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

  CrÃ©er fonction de validation continues:
  
  ```python
  def monitor_model_performance(predictions_file, actuals_file):
      """Valide performance sur donnÃ©es rÃ©elles"""
      
      pred = pd.read_csv(predictions_file)
      actual = pd.read_csv(actuals_file)
      
      # Calcul mÃ©triques
      auc = roc_auc_score(actual['risk'], pred['risk_prob'])
      recall = recall_score(actual['risk'], pred['risk_pred'])
      
      # Alerte si dÃ©rive
      if auc < 0.90:  # Seuil acceptable
          print("âš ï¸  MODEL DRIFT DETECTED: AUC =", auc)
          return False
      
      print(f"âœ… Model OK - AUC: {auc:.4f}, Recall: {recall:.4f}")
      return True
  
  # ExÃ©cuter mensuellement
  import schedule
  
  schedule.every().month.do(monitor_model_performance, 
                            'monthly_predictions.csv',
                            'monthly_actuals.csv')
  ```

  Retraining dÃ©clenchÃ© si dÃ©rive:
  
  ```python
  def retrain_if_needed(performance_ok):
      if not performance_ok:
          print("ðŸ”„ Launching retraining...")
          # Charger donnÃ©es + rÃ©entraÃ®ner
          # Sauvegarder nouveau modÃ¨le
          # Tester sur test set historique
          # Si meilleur â†’ dÃ©ployer
  ```

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
7. BEST PRACTICES
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

âœ… DO:
  â€¢ Toujours utiliser validation stratifiÃ©e (preserve class ratio)
  â€¢ Monitorer train/test gap pour dÃ©tecter overfitting
  â€¢ Sauvegarder le scaler avec le modÃ¨le
  â€¢ Documenter hyperparamÃ¨tres utilisÃ©s
  â€¢ Tracker performances en production
  â€¢ Retrain rÃ©guliÃ¨rement (3-6 mois)
  â€¢ Faire A/B testing des interventions

âŒ DON'T:
  â€¢ Ne pas oublier de normaliser inputs
  â€¢ Ne pas mÃ©langer train/test data
  â€¢ Ne pas tuner hyperparamÃ¨tres sur test set
  â€¢ Ne pas ignorer class imbalance
  â€¢ Ne pas dÃ©ployer sans validation cross-validation
  â€¢ Ne pas utiliser accuracy comme mÃ©trique principale

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
8. RÃ‰FÃ‰RENCES & RESSOURCES
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Papiers ClÃ©s:
  â€¢ GCN: Kipf & Welling (2017) "Semi-supervised Classification..."
  â€¢ GAT: VeliÄkoviÄ‡ et al. (2018) "Graph Attention Networks"
  â€¢ Class Imbalance: Lin et al. (2017) "Focal Loss for Dense Object Detection"

PyTorch Geometric Documentation:
  https://pytorch-geometric.readthedocs.io/

Graph Neural Networks Overview:
  https://distill.pub/2021/gnn-intro/

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Pour questions/support: Voir DOCUMENTATION_COMPLETE.md
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
